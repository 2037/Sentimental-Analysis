<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="">
  <meta name="author" content="Jack Wang">
  <link href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">

  <title>Sentiment Analysis: text to emoji</title>

  <style>
    .column {
      width: 500px;
      float: left
    }

    body {
      padding-top: 70px;
    }

    footer {
      margin: 50px 0;
    }
  </style>
</head>

<body>
  <div class="container">
    <!-- <div class="column"> -->
    <!-- Navigation -->

    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
          <a class="navbar-brand" href="#">Prediction of Emoji</a>
        </div>
      </div>
      <!-- /.container -->
    </nav>

    <!-- <h1>Prediction of Emoji</h1> -->
    <div class="row">
      <div class="col-lg-12">
        <h1 class="page-header">Prediction of Emoji
        </h1>
      </div>
    </div>
    This project is to test machine learning algorithms on prediction of emoji based on input text. It's like emojis pop up after texting a sentence in a smart phone.

    <h2 id="word2vec">Word2Vec</h2>
    <p>We use Glove (glove.6B.50d.txt) from Stanford for word embedding (like a dictionary from English to 50-d space).
      For each sentence, we slice and map the words into a embedding matrix where each row represent a 50-dimension word
      vector. Then we use cosine similarity to compute the semantic distance between words. We construct a LSTM model as
      follows: input layer(10-d as the max word counts of training set is 10) connects to the embedding, the second
      layer((10,50) is the vector ), then a dropout layer and lastly a dense layer that outputs five dimension vectors
      as we only have five emoji available for prediction. </p>
    <p>For the available models in scikit learn, we directly use the tfidf for word embedding. </p>

    <h2 id="bag-of-words-tf-idf">Bag of Words: TF-IDF</h2>
    <p>BoW simply counts the frequency of words in a document. Thus the vector for a document has the frequency of each
      word in the corpus for that document. In TF-IDF (term frequency-inverse document frequency) The importance of a
      word is proportional to its frequency in the text (TF) and inversely proportional to its frequency in the corpus
      (IDF).</p>
    <p>TF is the total number of times a word appears in the article. In order to eliminate the difference between
      different article sizes and facilitate the comparison between different articles, we normalize the term frequency
      here: TF = the total number of times a word appears in the article / the total number of words. </p>
    <p>IDF is the inverse document frequency. Inverse Document Frequency (IDF) = log(total number of documents in the
      corpus / number of documents containing the word + 1) (add 1 is to avoid a denominator of 0): TF-IDF = TF * IDF.
    </p>
    <p>TF-IDF is simple, computationally cheap, and easy to use, but TF-IDF cannot help carry word&#39;s meaning when
      handling complex or logical semantic context. For example, negation words like &quot;no&quot; should seprerate two
      phrases into completely opposite direction, but they&#39;re treated as only one words that appears once and not
      all phrases with negation are alike. However, it&#39;s enough to use to predict emoji here. </p>


    <div class="row">
      <div class="col-lg-12">
        <h1 class="page-header">Prediction on samples from test set
        </h1>
      </div>
    </div>

    {% for table in tables %}
    {{titles[loop.index]}}
    {{ table|safe }}
    {% endfor %}

    <form action="/" method="post">
      <input type="submit" value="Randomize" name="action" />
    </form>


    <div class="row">
      <div class="col-lg-12">
        <h1 class="page-header">Prediction on your words
        </h1>
      </div>
    </div>

    <!-- <h2>Prediction on your word</h2> -->
    <form action="/" method="post">
      <p><input type="utf-8" name="action2" value="Enter text here then hit enter"></p>
    </form>
    (Enter Text then hit enter)
    {% for table in tables_text %}
    {{titles_text[loop.index]}}
    {{ table|safe }}
    {% endfor %}
    <br>LSTM is disabled for its large size of model and set to predict heart. 
    <br>You can try some of following:
    <br>bad day, good morning, i like you, nice ball, i need food.

    <div class="row">
      <div class="col-lg-12">
        <h1 class="page-header">Accuracy performed in test set
        </h1>
      </div>
    </div>
    <!-- <h2>Accuracy performed in test set</h2> -->
    <table class="tg">
      <thead>
        <tr>
          <th class="tg-7zrl">Algorithm</th>
          <th class="tg-7zrl">Package</th>
          <th class="tg-7zrl">Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="tg-cly1">Multinomial Na√Øve Bayes</td>
          <td class="tg-cly1">sklearn.naive_bayes.MultinomialNB</td>
          <td class="tg-p9zu">51.79%</td>
        </tr>
        <tr>
          <td class="tg-cly1">Logistic Regression</td>
          <td class="tg-cly1">sklearn.linear_model.LogisticRegression</td>
          <td class="tg-p9zu">51.79%</td>
        </tr>
        <tr>
          <td class="tg-7zrl">Random Forest</td>
          <td class="tg-7zrl">sklearn.ensemble.RandomForestClassifier</td>
          <td class="tg-p9zu">58.93%</td>
        </tr>
        <tr>
          <td class="tg-7zrl">Gradient Boosting</td>
          <td class="tg-7zrl">sklearn.ensemble.GradientBoostingClassifier</td>
          <td class="tg-p9zu">62.50%</td>
        </tr>
        <tr>
          <td class="tg-7zrl">Extreme Gradient Boosting</td>
          <td class="tg-7zrl">xgboost.XGBClassifier</td>
          <td class="tg-p9zu">51.79%</td>
        </tr>
        <tr>
          <td class="tg-7zrl">Long Short Term Memory</td>
          <td class="tg-cly1">tensorflow.keras.layers.LSTM</td>
          <td class="tg-p9zu">53.57%</td>
        </tr>
      </tbody>
    </table>

    <div class="row">
      <div class="col-lg-12">
        <h1 class="page-header">Improvement
        </h1>
      </div>
    </div>


    The potential improvement is to increase training set. Increase features size since there're only five emoji
    involved and input sentences might have more than one reasonable output. We can also perform hyperparameter
    tuning. Gradient Boosting Classifier is the best algorithms amongs as it is just a classification problem. LSTM would be more useful if the text is longer and context meaningful.  

</body>

</div>

<!-- </div> -->

</html>